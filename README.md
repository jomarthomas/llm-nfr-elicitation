### **Leveraging Large Language Models for Automating Non-Functional Requirements Elicitation in Software Engineering: Enhancing Efficiency and Coverage**

#### **Project Overview**
This project explores how Large Language Models (LLMs) can be leveraged to automate the generation of Non-Functional Requirements (NFRs) from Functional Requirements (FRs). The goal is to improve the efficiency, coverage, and quality of NFR elicitation in software engineering projects.

The project integrates **ISO 25010**—a quality model for product quality evaluation—to ensure a standardized approach for assessing the generated requirements. Additionally, the study will include a **comparison of multiple LLMs**, each tested at **three different temperature settings**, to evaluate how model variability impacts the quality of the generated NFRs.

As part of this research, we will also **develop and propose a prompting technique** specifically tailored for the elicitation of NFRs. This will involve iterative refinements based on empirical testing and evaluations.

#### **Repository Includes:**
- **Preprocessing scripts** to prepare datasets.
- **JavaScript-based pipeline** for generating NFRs using LLMs.
- **Full-stack prototype** with a ReactJS frontend and a NodeJS backend.
- **Evaluation framework**, implementing metrics based on ISO 25010 to assess clarity, relevance, and coverage of generated NFRs.
- **Benchmarking experiments** comparing multiple LLMs with different temperature settings.

#### **Key Features:**
- **Automated NFR Elicitation:** Generates NFRs based on FR inputs using LLMs.
- **Full-Stack Prototype:** Provides a user-friendly interface for FR-to-NFR generation.
- **Evaluation Framework:** Uses ISO 25010 quality attributes to validate generated NFRs.
- **Scalable JavaScript Implementation:** Built entirely using JavaScript for seamless development.
- **LLM Benchmarking:** Compares different models and temperature settings to optimize performance.